{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WJ9haV-FLf4dzSiSLos1pA2hQZUm_OZF",
      "authorship_tag": "ABX9TyOF5ecZtp/yqQORC9if1nql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rbdus0715/information-retrieval/blob/main/boolean_retrieval/boolean_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[reference github](https://github.com/jin-zhe/boolean-retrieval-engine)</br>\n",
        "[dataset](https://www.kaggle.com/datasets/boldy717/reutersnltk)"
      ],
      "metadata": {
        "id": "-f_iZqPrSoQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/datasets/kaggle.nltk_reuters_dataset.data/archive.zip"
      ],
      "metadata": {
        "id": "qrM_aHE0TbjG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file = '/content/reutersNLTK.xlsx'\n",
        "df = pd.read_excel(file, engine='openpyxl')\n",
        "df = df['text']"
      ],
      "metadata": {
        "id": "lbsbfQa3TJfa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir document_dir"
      ],
      "metadata": {
        "id": "3WblNw9mTnto"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for index, row in df.items():\n",
        "    file_name = os.path.join('document_dir', f'{index+1}')\n",
        "    with open(file_name, 'w') as f:\n",
        "        f.write(row)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "h0esWTuQU2i5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_file = '/content/queries_file'\n",
        "posting_file = '/content/posting_file'\n",
        "\n",
        "query = 'fear OR rais'\n",
        "\n",
        "with open(queries_file, 'w') as f:\n",
        "    f.write(query)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "JQYsK2Oonfq-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch output_file"
      ],
      "metadata": {
        "id": "5JNqAP3foGUq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "pABACNEBXMzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7Z2ZWiC1Zf8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d8b982-2583-47c4-999a-e66327d0bb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import sys\n",
        "import getopt\n",
        "import codecs\n",
        "import os\n",
        "import struct\n",
        "import timeit\n",
        "\n",
        "LIMIT = None                # (for testing) to limit the number of documents indexed\n",
        "IGNORE_STOPWORDS = True     # toggling the option for ignoring stopwords\n",
        "IGNORE_NUMBERS = True       # toggling the option for ignoring numbers\n",
        "IGNORE_SINGLES = True       # toggling the option for ignoring single character tokens\n",
        "RECORD_TIME = False         # toggling for recording the time taken for indexer\n",
        "BYTE_SIZE = 4               # docID is in int\n",
        "\n",
        "document_directory = '/content/document_dir'\n",
        "dictionary_file = '/content/dictionary_file'\n",
        "posting_file = '/content/posting_file'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_number(token):\n",
        "    token = token.replace(\",\", \"\")  # ignore commas in token\n",
        "    # tries if token can be parsed as float\n",
        "    try:\n",
        "        float(token)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "E7RoRQt7wk_g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index(document_directory, dictionary_file, posting_file):\n",
        "    docID_list = [int(docID_string) for docID_string in os.listdir(document_directory)]\n",
        "    docID_list.sort()\n",
        "\n",
        "    stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    docs_indexed = 0\n",
        "    dictionary = {}\n",
        "\n",
        "    for docID in docID_list:\n",
        "        if LIMIT and docs_indexed == LIMIT: break\n",
        "        file_path = os.path.join(document_directory, str(docID))\n",
        "\n",
        "        file = codecs.open(file_path, encoding='utf-8')\n",
        "        document = file.read()                  # read entire document\n",
        "        tokens = nltk.word_tokenize(document)   # list of word tokens from document\n",
        "\n",
        "        # for each term in document\n",
        "        for word in tokens:\n",
        "            term = word.lower()         # casefolding\n",
        "            if (IGNORE_STOPWORDS and term in stopwords):    continue    # if ignoring stopwords\n",
        "            if (IGNORE_NUMBERS and is_number(term)):        continue    # if ignoring numbers\n",
        "            term = stemmer.stem(term)   # stemming\n",
        "            if (term[-1] == \"'\"):\n",
        "                term = term[:-1]        # remove apostrophe\n",
        "            if (IGNORE_SINGLES and len(term) == 1):         continue    # if ignoring single terms\n",
        "\n",
        "            if (term not in dictionary):\n",
        "                dictionary[term] = [docID]\n",
        "            else:\n",
        "                if (dictionary[term][-1] != docID):\n",
        "                    dictionary[term].append(docID)\n",
        "\n",
        "        docs_indexed += 1\n",
        "        file.close()\n",
        "\n",
        "    dict_file = codecs.open(dictionary_file, 'w', encoding='utf-8')\n",
        "    post_file = open(posting_file, 'wb')\n",
        "\n",
        "    byte_offset = 0\n",
        "\n",
        "    # write list of docIDs indexed to first line of dictionary\n",
        "    dict_file.write('Indexed from docIDs:')\n",
        "    for i in range(docs_indexed):\n",
        "        dict_file.write(str(docID_list[i]) + ',')\n",
        "    dict_file.write('\\n')\n",
        "\n",
        "    # build dictionary file and postings file\n",
        "    for term, postings_list in dictionary.items():\n",
        "        df = len(postings_list)                     # document frequency is the same as length of postings list\n",
        "\n",
        "        # write each posting into postings file\n",
        "        for docID in postings_list:\n",
        "            posting = struct.pack('I', docID)   # pack docID into a byte array of size 4\n",
        "            post_file.write(posting)\n",
        "\n",
        "        # write to dictionary file and update byte offset\n",
        "        dict_file.write(term + \" \" + str(df) + \" \" + str(byte_offset) + \"\\n\")\n",
        "        byte_offset += BYTE_SIZE * df\n",
        "\n",
        "    # close files\n",
        "    dict_file.close()\n",
        "    post_file.close()"
      ],
      "metadata": {
        "id": "O8X8XMUxZjd3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index(document_directory, dictionary_file, posting_file)"
      ],
      "metadata": {
        "id": "8gNkmVlUyULG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "search"
      ],
      "metadata": {
        "id": "isOXqlcphJQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import sys\n",
        "import getopt\n",
        "import codecs\n",
        "import struct\n",
        "import math\n",
        "import io\n",
        "import collections\n",
        "import timeit\n",
        "\n",
        "RECORD_TIME = False # toggling for recording the time taken for indexer\n",
        "BYTE_SIZE = 4       # docID is in int"
      ],
      "metadata": {
        "id": "utYnh1GyyvgN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dictionary(dict_file):\n",
        "    dictionary = {}                 # dictionary map loaded\n",
        "    indexed_docIDs = []             # list of all docIDs indexed\n",
        "    docIDs_processed = False        # if indexed_docIDs is processed\n",
        "\n",
        "    # load each term along with its df and postings file pointer to dictionary\n",
        "    for entry in dict_file.read().split('\\n'):\n",
        "        # if entry is not empty (last line in dictionary file is empty)\n",
        "        if (entry):\n",
        "            # if first line of dictionary, process list of docIDs indexed\n",
        "            if (not docIDs_processed):\n",
        "                indexed_docIDs = [int(docID) for docID in entry[20:-1].split(',')]\n",
        "                docIDs_processed = True\n",
        "            # else if dictionary terms and their attributes\n",
        "            else:\n",
        "                token = entry.split(\" \")\n",
        "                term = token[0]\n",
        "                df = int(token[1])\n",
        "                offset = int(token[2])\n",
        "                dictionary[term] = (df, offset)\n",
        "\n",
        "    return (dictionary, indexed_docIDs)"
      ],
      "metadata": {
        "id": "cO8sAMf1hPku"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_posting_list(post_file, length, offset):\n",
        "    post_file.seek(offset)\n",
        "    posting_list = []\n",
        "    for i in range(length):\n",
        "        posting = post_file.read(BYTE_SIZE)\n",
        "        docID = struct.unpack('I', posting)[0]\n",
        "        posting_list.append(docID)\n",
        "    return posting_list\n",
        "\n",
        "def shunting_yard(infix_tokens):\n",
        "    # define precedences\n",
        "    precedence = {}\n",
        "    precedence['NOT'] = 3\n",
        "    precedence['AND'] = 2\n",
        "    precedence['OR'] = 1\n",
        "    precedence['('] = 0\n",
        "    precedence[')'] = 0\n",
        "\n",
        "    # declare data strucures\n",
        "    output = []\n",
        "    operator_stack = []\n",
        "\n",
        "    # while there are tokens to be read\n",
        "    for token in infix_tokens:\n",
        "\n",
        "        # if left bracket\n",
        "        if (token == '('):\n",
        "            operator_stack.append(token)\n",
        "\n",
        "        # if right bracket, pop all operators from operator stack onto output until we hit left bracket\n",
        "        elif (token == ')'):\n",
        "            operator = operator_stack.pop()\n",
        "            while operator != '(':\n",
        "                output.append(operator)\n",
        "                operator = operator_stack.pop()\n",
        "\n",
        "        # if operator, pop operators from operator stack to queue if they are of higher precedence\n",
        "        elif (token in precedence):\n",
        "            # if operator stack is not empty\n",
        "            if (operator_stack):\n",
        "                current_operator = operator_stack[-1]\n",
        "                while (operator_stack and precedence[current_operator] > precedence[token]):\n",
        "                    output.append(operator_stack.pop())\n",
        "                    if (operator_stack):\n",
        "                        current_operator = operator_stack[-1]\n",
        "\n",
        "            operator_stack.append(token) # add token to stack\n",
        "\n",
        "        # else if operands, add to output list\n",
        "        else:\n",
        "            output.append(token.lower())\n",
        "\n",
        "    # while there are still operators on the stack, pop them into the queue\n",
        "    while (operator_stack):\n",
        "        output.append(operator_stack.pop())\n",
        "    # print ('postfix:', output)  # check\n",
        "    return output"
      ],
      "metadata": {
        "id": "GSdjn5V3hW9M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_NOT(right_operand, indexed_docIDs):\n",
        "    # complement of an empty list is list of all indexed docIDs\n",
        "    if (not right_operand):\n",
        "        return indexed_docIDs\n",
        "\n",
        "    result = []\n",
        "    r_index = 0 # index for right operand\n",
        "    for item in indexed_docIDs:\n",
        "        # if item do not match that in right_operand, it belongs to compliment\n",
        "        if (item != right_operand[r_index]):\n",
        "            result.append(item)\n",
        "        # else if item matches and r_index still can progress, advance it by 1\n",
        "        elif (r_index + 1 < len(right_operand)):\n",
        "            r_index += 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def boolean_OR(left_operand, right_operand):\n",
        "    result = []     # union of left and right operand\n",
        "    l_index = 0     # current index in left_operand\n",
        "    r_index = 0     # current index in right_operand\n",
        "\n",
        "    # while lists have not yet been covered\n",
        "    while (l_index < len(left_operand) or r_index < len(right_operand)):\n",
        "        # if both list are not yet exhausted\n",
        "        if (l_index < len(left_operand) and r_index < len(right_operand)):\n",
        "            l_item = left_operand[l_index]  # current item in left_operand\n",
        "            r_item = right_operand[r_index] # current item in right_operand\n",
        "\n",
        "            # case 1: if items are equal, add either one to result and advance both pointers\n",
        "            if (l_item == r_item):\n",
        "                result.append(l_item)\n",
        "                l_index += 1\n",
        "                r_index += 1\n",
        "\n",
        "            # case 2: l_item greater than r_item, add r_item and advance r_index\n",
        "            elif (l_item > r_item):\n",
        "                result.append(r_item)\n",
        "                r_index += 1\n",
        "\n",
        "            # case 3: l_item lower than r_item, add l_item and advance l_index\n",
        "            else:\n",
        "                result.append(l_item)\n",
        "                l_index += 1\n",
        "\n",
        "        # if left_operand list is exhausted, append r_item and advance r_index\n",
        "        elif (l_index >= len(left_operand)):\n",
        "            r_item = right_operand[r_index]\n",
        "            result.append(r_item)\n",
        "            r_index += 1\n",
        "\n",
        "        # else if right_operand list is exhausted, append l_item and advance l_index\n",
        "        else:\n",
        "            l_item = left_operand[l_index]\n",
        "            result.append(l_item)\n",
        "            l_index += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def boolean_AND(left_operand, right_operand):\n",
        "    # perform 'merge'\n",
        "    result = []                                 # results list to be returned\n",
        "    l_index = 0                                 # current index in left_operand\n",
        "    r_index = 0                                 # current index in right_operand\n",
        "    l_skip = int(math.sqrt(len(left_operand)))  # skip pointer distance for l_index\n",
        "    r_skip = int(math.sqrt(len(right_operand))) # skip pointer distance for r_index\n",
        "\n",
        "    while (l_index < len(left_operand) and r_index < len(right_operand)):\n",
        "        l_item = left_operand[l_index]  # current item in left_operand\n",
        "        r_item = right_operand[r_index] # current item in right_operand\n",
        "\n",
        "        # case 1: if match\n",
        "        if (l_item == r_item):\n",
        "            result.append(l_item)   # add to results\n",
        "            l_index += 1            # advance left index\n",
        "            r_index += 1            # advance right index\n",
        "\n",
        "        # case 2: if left item is more than right item\n",
        "        elif (l_item > r_item):\n",
        "            # if r_index can be skipped (if new r_index is still within range and resulting item is <= left item)\n",
        "            if (r_index + r_skip < len(right_operand)) and right_operand[r_index + r_skip] <= l_item:\n",
        "                r_index += r_skip\n",
        "            # else advance r_index by 1\n",
        "            else:\n",
        "                r_index += 1\n",
        "\n",
        "        # case 3: if left item is less than right item\n",
        "        else:\n",
        "            # if l_index can be skipped (if new l_index is still within range and resulting item is <= right item)\n",
        "            if (l_index + l_skip < len(left_operand)) and left_operand[l_index + l_skip] <= r_item:\n",
        "                l_index += l_skip\n",
        "            # else advance l_index by 1\n",
        "            else:\n",
        "                l_index += 1\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "As-1BufGhaIg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query, dictionary, post_file, indexed_docIDs):\n",
        "    stemmer = nltk.stem.porter.PorterStemmer() # instantiate stemmer\n",
        "    # prepare query list\n",
        "    query = query.replace('(', '( ')\n",
        "    query = query.replace(')', ' )')\n",
        "    query = query.split(' ')\n",
        "\n",
        "    results_stack = []\n",
        "    postfix_queue = collections.deque(shunting_yard(query)) # get query in postfix notation as a queue\n",
        "\n",
        "    while postfix_queue:\n",
        "        token = postfix_queue.popleft()\n",
        "        result = [] # the evaluated result at each stage\n",
        "        # if operand, add postings list for term to results stack\n",
        "        if (token != 'AND' and token != 'OR' and token != 'NOT'):\n",
        "            token = stemmer.stem(token) # stem the token\n",
        "            # default empty list if not in dictionary\n",
        "            if (token in dictionary):\n",
        "                result = load_posting_list(post_file, dictionary[token][0], dictionary[token][1])\n",
        "\n",
        "        # else if AND operator\n",
        "        elif (token == 'AND'):\n",
        "            right_operand = results_stack.pop()\n",
        "            left_operand = results_stack.pop()\n",
        "            # print(left_operand, 'AND', left_operand) # check\n",
        "            result = boolean_AND(left_operand, right_operand)   # evaluate AND\n",
        "\n",
        "        # else if OR operator\n",
        "        elif (token == 'OR'):\n",
        "            right_operand = results_stack.pop()\n",
        "            left_operand = results_stack.pop()\n",
        "            # print(left_operand, 'OR', left_operand) # check\n",
        "            result = boolean_OR(left_operand, right_operand)    # evaluate OR\n",
        "\n",
        "        # else if NOT operator\n",
        "        elif (token == 'NOT'):\n",
        "            right_operand = results_stack.pop()\n",
        "            # print('NOT', right_operand) # check\n",
        "            result = boolean_NOT(right_operand, indexed_docIDs) # evaluate NOT\n",
        "\n",
        "        # push evaluated result back to stack\n",
        "        results_stack.append(result)\n",
        "        # print ('result', result) # check\n",
        "\n",
        "    # NOTE: at this point results_stack should only have one item and it is the final result\n",
        "    if len(results_stack) != 1: print (\"ERROR: results_stack. Please check valid query\") # check for errors\n",
        "\n",
        "    return results_stack.pop()\n"
      ],
      "metadata": {
        "id": "gVCjMKLnhQOM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(dictionary_file, postings_file, queries_file, output_file):\n",
        "    # open files\n",
        "    dict_file = codecs.open(dictionary_file, encoding='utf-8')\n",
        "    post_file = io.open(postings_file, 'rb')\n",
        "    query_file = codecs.open(queries_file, encoding='utf-8')\n",
        "    out_file = open(output_file, 'w')\n",
        "\n",
        "    # load dictionary to memory\n",
        "    loaded_dict = load_dictionary(dict_file)\n",
        "    dictionary = loaded_dict[0]     # dictionary map\n",
        "    indexed_docIDs = loaded_dict[1] # list of all docIDs indexed in sorted order\n",
        "    dict_file.close()\n",
        "\n",
        "    # process each query\n",
        "    queries_list = query_file.read().splitlines()\n",
        "    for i in range(len(queries_list)):\n",
        "        query = queries_list[i]\n",
        "        result = process_query(query, dictionary, post_file, indexed_docIDs)\n",
        "        # write each result to output\n",
        "        for j in range(len(result)):\n",
        "            docID = str(result[j])\n",
        "            if (j != len(result) - 1):\n",
        "                docID += ' '\n",
        "            out_file.write(docID)\n",
        "        if (i != len(queries_list) - 1):\n",
        "            out_file.write('\\n')\n",
        "\n",
        "    # close files\n",
        "    post_file.close()\n",
        "    query_file.close()\n",
        "    out_file.close()"
      ],
      "metadata": {
        "id": "arvldwDlhLtZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_file = '/content/queries_file'\n",
        "output_file = '/content/output_file'\n",
        "\n",
        "search(dictionary_file, posting_file, queries_file, output_file)"
      ],
      "metadata": {
        "id": "ZTZ0vUUDhfJS"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}